<!DOCTYPE html> <html lang="fr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Dinh-Viet-Toan Le </title> <meta name="author" content="Dinh-Viet-Toan Le"> <meta name="description" content="Un thème simple et épuré pour les universitaires. Basé sur le design [*folio](https://github.com/bogoli/-folio)."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%BC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dinhviettoanle.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/fr/"> <span class="font-weight-bold">Dinh-Viet-Toan</span> Le </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/fr/">à propos </a> </li> <li class="nav-item active"> <a class="nav-link" href="/fr/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/fr/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/fr/teaching/">enseignement </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/"> EN-US</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title text-capitalize">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Filtrer"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> Symbolic<br>Music Generation </abbr> <figure> <picture> <img src="/assets/img/publication_preview/meteor.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="meteor.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2025meteor" class="col-sm-8"> <div class="title">METEOR: Melody-aware Texture-controllable Symbolic Music Re-Orchestration via Transformer VAE</div> <div class="author"> <em>Dinh-Viet-Toan Le</em>, and <a href="https://affige.github.io/" rel="external nofollow noopener" target="_blank">Yi-Hsuan Yang</a> </div> <div class="periodical"> <em>In <a href="https://2025.ijcai.org/" class="conf_link" rel="external nofollow noopener" target="_blank">Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI-25</a></em>, Aug 2025 </div> <div class="periodical"> AI, Arts &amp; Creativity </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.24963/ijcai.2025/1125" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/pdf/2409.11753" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/dinhviettoanle/meteor" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dinhviettoanle.github.io/fr/meteor/" class="btn btn-sm z-depth-0" role="button">Demo</a> </div> <div class="abstract hidden"> <p>Re-orchestration is the process of adapting a music piece for a different set of instruments. By altering the original instrumentation, the orchestrator often modifies the musical texture while preserving a recognizable melodic line and ensures that each part is playable within the technical and expressive capabilities of the chosen instruments. In this work, we propose METEOR, a model for generating Melody-aware Texture-controllable re-Orchestration with a Transformer-based variational auto-encoder (VAE). This model performs symbolic instrumental and textural music style transfers with a focus on melodic fidelity and controllability. We allow bar- and track-level controllability of the accompaniment with various textural attributes while keeping a homophonic texture. With both subjective and objective evaluations, we show that our model outperforms style transfer models on a re-orchestration task in terms of generation quality and controllability. Moreover, it can be adapted for a lead sheet orchestration task as a zero-shot learning model, achieving performance comparable to a model specifically trained for this task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b8815c"> Musicology </abbr> <figure> <picture> <img src="/assets/img/publication_preview/dezrann.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dezrann.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ballester2025dezrann" class="col-sm-8"> <div class="title">Interacting with Annotated and Synchronized Music Corpora on the Dezrann Web Platform</div> <div class="author"> Charles Ballester, Baptiste Bacot, <a href="https://louisbigo.com/" rel="external nofollow noopener" target="_blank">Louis Bigo</a>, Vanessa Nina Borsan, Louis Couturier, and <span class="more-authors" title="click to view 21 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '21 more authors' ? 'Ken Déguernel, Quentin Dinel, Laurent Feisthauer, Klaus Frieler, Mark Gotham, Richard Groult, Johannes Hentschel, Alexandre D’Hooge, Dinh-Viet-Toan Le, Florence Levé, Francesco Maccarini, Ivana Maričić, Gianluca Micchi, Meinard Müller, Alexandros Stamatiadis, Tom Taffin, Patrice Thibaud, Christof Weiß, Rui Yang, Emmanuel Leguy, Mathieu Giraud' : '21 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">21 more authors</span> </div> <div class="periodical"> <em>Transactions of the International Society for Music Information Retrieval (TISMIR)</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5334/tismir.212" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://transactions.ismir.net/articles/212/files/68720942a2a8e.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://dezrann.net/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Demo</a> </div> <div class="abstract hidden"> <p>Open datasets with annotated corpora are crucial to foster research in music information retrieval (MIR) studies and to disseminate knowledge towards musicians and the general public. Many groups have published corpora, at times accompanied by specific visualization interfaces. However, achieving a cohesive access to such a diverse range of data poses a significant challenge. Dezrann is an open-source web platform to interact with corpora while sharing music and music analysis in the form of scores, images, audio files (waveforms), video files, and annotations. We present how we render through this platform ten curated corpora published in the last years in the MIR community, gathering 1500+ pieces and 35000+ annotations. These corpora include works by Bach, Mozart, Beethoven, and Schubert, lieder from the 19th century by female composers (OpenScore Lieder), jazz solo transcriptions (Weimar Jazz Database), piano rolls (SUPRA), Georgian sacred songs (Erkomaishvili dataset), and Slovenian folk song ballads. Showing these corpora with the cross-modal synchronization enabled by the platform improves the way to hear, study, and annotate music. This opens up new possibilities for corpus annotation and analysis in musicology and computer music research, enhances music education, fosters the promotion of music diversity, and facilitates collaborative score editing and correction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ballester2025dezrann</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Interacting with Annotated and Synchronized Music Corpora on the Dezrann Web Platform}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ballester, Charles and Bacot, Baptiste and Bigo, Louis and Borsan, Vanessa Nina and Couturier, Louis and D{\'e}guernel, Ken and Dinel, Quentin and Feisthauer, Laurent and Frieler, Klaus and Gotham, Mark and Groult, Richard and Hentschel, Johannes and D'Hooge, Alexandre and Le, Dinh-Viet-Toan and Lev{\'e}, Florence and Maccarini, Francesco and Mari{\v c}i{\'c}, Ivana and Micchi, Gianluca and M{\"u}ller, Meinard and Stamatiadis, Alexandros and Taffin, Tom and Thibaud, Patrice and Wei{\ss}, Christof and Yang, Rui and Leguy, Emmanuel and Giraud, Mathieu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Transactions of the International Society for Music Information Retrieval (TISMIR)}}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{121--139}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Ubiquity Press}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hal.science/hal-04956003}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5334/tismir.212}</span><span class="p">,</span>
  <span class="na">hal_local_reference</span> <span class="p">=</span> <span class="s">{SH8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Music annotation ; Reproducibility ; Music ; Representations ; Web platform ; Cross-modal synchronization ; Score ; Audio ; Music corporameta-corpus}</span><span class="p">,</span>
  <span class="na">hal_id</span> <span class="p">=</span> <span class="s">{hal-04956003}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#6899a6"> Symbolic<br>Music Analysis </abbr> <figure> <picture> <img src="/assets/img/publication_preview/intervalization.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="intervalization.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2025evaluating" class="col-sm-8"> <div class="title">Evaluating Interval-based Tokenization for Pitch Representation in Symbolic Music Analysis</div> <div class="author"> <em>Dinh-Viet-Toan Le</em>, <a href="https://louisbigo.com/" rel="external nofollow noopener" target="_blank">Louis Bigo</a>, and <a href="https://www.cristal.univ-lille.fr/en/profil/kellerm/" rel="external nofollow noopener" target="_blank">Mikaela Keller</a> </div> <div class="periodical"> <em>In <a href="https://ai4musicians.org/2025aaai.html" class="conf_link" rel="external nofollow noopener" target="_blank">Workshop Artificial Intelligence for Music at AAAI</a></em>, Philadelphia, United States, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://hal.science/hal-04877659v1/document" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Symbolic music analysis tasks are often performed by models originally developed for Natural Language Processing, such as Transformers. Such models require the input data to be represented as sequences, which is achieved through a process of tokenization. Tokenization strategies for symbolic music often rely on absolute MIDI values to represent pitch information. However, music research largely promotes the benefit of higher-level representations such as melodic contour and harmonic relations for which pitch intervals turn out to be more expressive than absolute pitches. In this work, we introduce a general framework for building interval-based tokenizations. By evaluating these tokenizations on three music analysis tasks, we show that such interval-based tokenizations improve model performances and facilitate their explainability</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">le2025evaluating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluating Interval-based Tokenization for Pitch Representation in Symbolic Music Analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le, Dinh-Viet-Toan and Bigo, Louis and Keller, Mikaela}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop Artificial Intelligence for Music at AAAI}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Philadelphia, United States}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Survey</abbr> <figure> <picture> <img src="/assets/img/publication_preview/survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2025natural" class="col-sm-8"> <div class="title">Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A Survey</div> <div class="author"> <em>Dinh-Viet-Toan Le</em>, <a href="https://louisbigo.com/" rel="external nofollow noopener" target="_blank">Louis Bigo</a>, <a href="https://dorienherremans.com/" rel="external nofollow noopener" target="_blank">Dorien Herremans</a>, and <a href="https://www.cristal.univ-lille.fr/en/profil/kellerm/" rel="external nofollow noopener" target="_blank">Mikaela Keller</a> </div> <div class="periodical"> <em>ACM Computing Surveys</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3714457" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3714457" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/dinhviettoanle/survey-music-nlp" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Music is frequently associated with the notion of language, as both domains share several similarities, including the ability for their content to be represented as sequences of symbols. In computer science, the fields of Natural Language Processing (NLP) and Music Information Retrieval (MIR) reflect this analogy through a variety of similar tasks, such as author detection or content generation. This similarity has long encouraged the adaptation of NLP methods to process musical data, particularly symbolic music data, and the rise of Transformer neural networks has considerably strengthened this practice. This survey reviews NLP methods applied to symbolic music generation and information retrieval following two axes. We first propose an overview of representations of symbolic music inspired by text sequential representations. We then review a large set of computational models, particularly deep learning models, which have been adapted from NLP to process these musical representations for various MIR tasks. These models are described and categorized through different prisms with a highlight on their music-specialized mechanisms. We finally present a discussion surrounding the adequate use of NLP tools to process symbolic music data. This includes technical issues regarding NLP methods which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">le2025natural</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le, Dinh-Viet-Toan and Bigo, Louis and Herremans, Dorien and Keller, Mikaela}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A Survey}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{July 2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{57}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0360-0300}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3714457}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3714457}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Computing Surveys}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{175}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{40}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Music information retrieval, natural language processing, symbolic music, music generation, music analysis, deep learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#6899a6"> Symbolic<br>Music Analysis </abbr> <figure> <picture> <img src="/assets/img/publication_preview/bpe.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bpe.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2024analyzing" class="col-sm-8"> <div class="title">Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic Music: A Focus on Musical Phrase Segmentation</div> <div class="author"> <em>Dinh-Viet-Toan Le</em>, <a href="https://louisbigo.com/" rel="external nofollow noopener" target="_blank">Louis Bigo</a>, and <a href="https://www.cristal.univ-lille.fr/en/profil/kellerm/" rel="external nofollow noopener" target="_blank">Mikaela Keller</a> </div> <div class="periodical"> <em>In <a href="https://sites.google.com/view/nlp4musa-2024" class="conf_link" rel="external nofollow noopener" target="_blank">Proceedings of the 3rd Workshop on NLP for Music and Audio (NLP4MusA)</a></em>, Oakland, United States, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.nlp4musa-1.12.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/dinhviettoanle/musicbpe-mono-poly" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language Processing to build a vocabulary of subwords, which has been recently applied to symbolic music. Given that symbolic music can differ significantly from text, particularly with polyphony, we investigate how BPE behaves with different types of musical content. This study provides a qualitative analysis of BPE‘s behavior across various instrumentations and evaluates its impact on a musical phrase segmentation task for both monophonic and polyphonic music. Our findings show that the BPE training process is highly dependent on the instrumentation and that BPE “supertokens” succeed in capturing abstract musical content. In a musical phrase segmentation task, BPE notably improves performance in a polyphonic setting, but enhances performance in monophonic tunes only within a specific range of BPE merges.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">le2024analyzing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic Music: A Focus on Musical Phrase Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le, Dinh-Viet-Toan and Bigo, Louis and Keller, Mikaela}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Kruspe, Anna and Oramas, Sergio and Epure, Elena V. and Sordo, Mohamed and Weck, Benno and Doh, SeungHeon and Won, Minz and Manco, Ilaria and Meseguer-Brocal, Gabriel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 3rd Workshop on NLP for Music and Audio (NLP4MusA)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Oakland, United States}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Lingustics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.nlp4musa-1.12/}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{69--74}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#bbbbbb"> General public </abbr> <figure> <picture> <img src="/assets/img/publication_preview/culture_recherche.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="culture_recherche.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bigo2024langage" class="col-sm-8"> <div class="title">Le langage des partitions musicales face à l’intelligence artificielle</div> <div class="author"> <a href="https://louisbigo.com/" rel="external nofollow noopener" target="_blank">Louis Bigo</a>, <a href="https://www.cristal.univ-lille.fr/en/profil/kellerm/" rel="external nofollow noopener" target="_blank">Mikaela Keller</a>, and <em>Dinh-Viet-Toan Le</em> </div> <div class="periodical"> <em>Culture et recherche</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://hal.science/hal-04909478/document" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Dans le domaine de l’informatique musicale, l’analyse et la génération automatique de partitions font l’objet de multiples recherches, en grande partie inspirées par des algorithmes d’Intelligence artificielle (IA) conçus à l’origine pour le traitement de contenus textuels. Le projet Music NLP, collaboration entre le Studio de création et de recherche en informatique et musiques expérimentales (SCRIME) à l’Université de Bordeaux, l’Institut national de recherche en sciences et technologies du numérique (INRIA) et le Centre de recherche en informatique signal et automatique de Lille (CRIStAL) à l’Université de Lille, mène une réflexion sur ce détournement d’outils, ses limites techniques et la manière dont il nous interroge à un plus haut niveau sur le parallèle entre musique et langage naturel. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bigo2024langage</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Le langage des partitions musicales face {\`a} l'intelligence artificielle}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bigo, Louis and Keller, Mikaela and Le, Dinh-Viet-Toan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Culture et recherche}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{34--35}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Paris : Minist{\`e}re de la Culture et de la Communication}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hal.science/hal-04909478}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Intelligence artificielle ; partitions musicales ; analyse musicale ; composition musicale ; langage naturel}</span><span class="p">,</span>
  <span class="na">hal_id</span> <span class="p">=</span> <span class="s">{hal-04909478}</span><span class="p">,</span>
  <span class="na">hal_version</span> <span class="p">=</span> <span class="s">{v1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b8815c"> Musicology </abbr> <figure> <picture> <img src="/assets/img/publication_preview/orchestral_texture.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="orchestral_texture.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2022orchestral" class="col-sm-8"> <div class="title">A Corpus Describing Orchestral Texture in First Movements of Classical and Early-Romantic Symphonies</div> <div class="author"> <em>Dinh-Viet-Toan Le</em>, <a href="https://cnrs.magiraud.org/" rel="external nofollow noopener" target="_blank">Mathieu Giraud</a>, <a href="https://home.mis.u-picardie.fr/~leve/" rel="external nofollow noopener" target="_blank">Florence Levé</a>, and Francesco Maccarini </div> <div class="periodical"> <em>In <a href="https://dlfm.web.ox.ac.uk/9th-international-conference-on-digital-libraries-for-musicology" class="conf_link" rel="external nofollow noopener" target="_blank">Proceedings of the 9th International Conference on Digital Libraries for Musicology</a></em>, Prague, Czech Republic, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3543882.3543884" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://hal.science/hal-03663112/document" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://gitlab.com/algomus.fr/orchestration" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Orchestration is the art of writing music for a possibly large ensemble of instruments, by blending or opposing their sounds and grouping them into an orchestral texture. We aim here at providing a deeper understanding of orchestration in classical and early-romantic symphonies by analyzing, at the bar level, how the instruments of the orchestra organize into melodic, rhythmic, harmonic, and mixed layers. We formalize the description of such layers and release an open corpus with more than 7900 annotations in 24 first movements of Haydn, Mozart, and Beethoven symphonies. Initial analyses of this corpus confirm specific roles of the instruments and their families (woodwinds, brass, and strings), some evolution between composers, as well as the contribution of orchestral texture to form. The model and the corpus offer perspectives for empirical and computational studies on orchestral music.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">le2022orchestral</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Corpus Describing Orchestral Texture in First Movements of Classical and Early-Romantic Symphonies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le, Dinh-Viet-Toan and Giraud, Mathieu and Lev\'{e}, Florence and Maccarini, Francesco}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 9th International Conference on Digital Libraries for Musicology}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{27-35}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Prague, Czech Republic}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450396684}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3543882.3543884}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3543882.3543884}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{corpus, layers, music texture, orchestration, symbolic data}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{DLfM '22}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dinh-Viet-Toan Le. Propulsé par <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> avec le thème <a href="https://github.com/george-gca/multi-language-al-folio" rel="external nofollow noopener" target="_blank">multi-language-al-folio</a>. Hébergé par <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Tapez pour commencer à rechercher"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> pour sélectionner </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> pour naviguer </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> pour fermer </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> remonter au parent </span> </div> </ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-\xe0-propos",title:"\xe0 propos",section:"Menu de navigation",handler:()=>{window.location.href="/fr/"}},{id:"nav-publications",title:"publications",description:"",section:"Menu de navigation",handler:()=>{window.location.href="/fr/publications/"}},{id:"nav-cv",title:"cv",description:"",section:"Menu de navigation",handler:()=>{window.location.href="/fr/cv/"}},{id:"nav-enseignement",title:"enseignement",description:"",section:"Menu de navigation",handler:()=>{window.location.href="/fr/teaching/"}},{id:"social-email",title:"Envoyer un e-mail",section:"R\xe9seaux sociaux",handler:()=>{window.open("mailto:%64%69%6E%68%76%69%65%74%74%6F%61%6E.%6C%65@%75%6E%69%76-%6C%69%6C%6C%65.%66%72","_blank")}},{id:"social-github",title:"GitHub",section:"R\xe9seaux sociaux",handler:()=>{window.open("https://github.com/dinhviettoanle","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"R\xe9seaux sociaux",handler:()=>{window.open("https://www.linkedin.com/in/dinh-viet-toan-le","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"R\xe9seaux sociaux",handler:()=>{window.open("https://scholar.google.com/citations?user=d5bdg88AAAAJ","_blank")}},{id:"social-semanticscholar",title:"Semantic Scholar",section:"R\xe9seaux sociaux",handler:()=>{window.open("https://www.semanticscholar.org/author/2179118216","_blank")}},{id:"lang-en-us",title:"en-us",section:"Langues",handler:()=>{window.location.href="/publications/"}},{id:"light-theme",title:"Passer au th\xe8me clair",description:"Changer le th\xe8me du site en clair",section:"Th\xe8me",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Passer au th\xe8me sombre",description:"Changer le th\xe8me du site en sombre",section:"Th\xe8me",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Utiliser le th\xe8me par d\xe9faut du syst\xe8me",description:"Changer le th\xe8me du site en fonction du syst\xe8me",section:"Th\xe8me",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>