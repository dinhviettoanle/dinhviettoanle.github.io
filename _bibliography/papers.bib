---
---

% ======================================================================
% ========================= Mes "publications"==========================
% ======================================================================

@article{ballester2025dezrann,
  bibtex_show  = {true},
  title        = {{Interacting with Annotated and Synchronized Music Corpora on the Dezrann Web Platform}},
  author       = {Ballester, Charles and Bacot, Baptiste and Bigo, Louis and Borsan, Vanessa Nina and Couturier, Louis and D{\'e}guernel, Ken and Dinel, Quentin and Feisthauer, Laurent and Frieler, Klaus and Gotham, Mark and Groult, Richard and Hentschel, Johannes and D'Hooge, Alexandre and Le, Dinh-Viet-Toan and Lev{\'e}, Florence and Maccarini, Francesco and Mari{\v c}i{\'c}, Ivana and Micchi, Gianluca and M{\"u}ller, Meinard and Stamatiadis, Alexandros and Taffin, Tom and Thibaud, Patrice and Wei{\ss}, Christof and Yang, Rui and Leguy, Emmanuel and Giraud, Mathieu},
  year         = 2025,
  month        = may,
  journal      = {{Transactions of the International Society for Music Information Retrieval (TISMIR)}},
  number       = 1,
  volume       = 8,
  pages        = {121--139},
  publisher    = {{Ubiquity Press}},
  url          = {https://hal.science/hal-04956003},
  doi          = {10.5334/tismir.212},
  hal_local_reference = {SH8},
  keywords     = {Music annotation ; Reproducibility ; Music ; Representations ; Web platform ; Cross-modal synchronization ; Score ; Audio ; Music corporameta-corpus},
  hal_id       = {hal-04956003},
  abstract     = {Open datasets with annotated corpora are crucial to foster research in music information retrieval (MIR) studies and to disseminate knowledge towards musicians and the general public. Many groups have published corpora, at times accompanied by specific visualization interfaces. However, achieving a cohesive access to such a diverse range of data poses a significant challenge. Dezrann is an open-source web platform to interact with corpora while sharing music and music analysis in the form of scores, images, audio files (waveforms), video files, and annotations. We present how we render through this platform ten curated corpora published in the last years in the MIR community, gathering 1500+ pieces and 35000+ annotations. These corpora include works by Bach, Mozart, Beethoven, and Schubert, lieder from the 19th century by female composers (OpenScore Lieder), jazz solo transcriptions (Weimar Jazz Database), piano rolls (SUPRA), Georgian sacred songs (Erkomaishvili dataset), and Slovenian folk song ballads. Showing these corpora with the cross-modal synchronization enabled by the platform improves the way to hear, study, and annotate music. This opens up new possibilities for corpus annotation and analysis in musicology and computer music research, enhances music education, fosters the promotion of music diversity, and facilitates collaborative score editing and correction.},
  abbr         = {Musicology},
  pdf          = {https://transactions.ismir.net/articles/212/files/68720942a2a8e.pdf},
  demo         = {https://dezrann.net/},
  preview      = {dezrann.png}
}


@inproceedings{le2024analyzing,
  bibtex_show = {true},
  title       = {Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic Music: A Focus on Musical Phrase Segmentation},
  author      = {Le, Dinh-Viet-Toan and Bigo, Louis and Keller, Mikaela},
  editor      = {Kruspe, Anna and Oramas, Sergio and Epure, Elena V. and Sordo, Mohamed and Weck, Benno and Doh, SeungHeon and Won, Minz and Manco, Ilaria and Meseguer-Brocal, Gabriel},
  booktitle   = {Proceedings of the 3rd Workshop on NLP for Music and Audio (NLP4MusA)},
  month       = nov,
  year        = 2024,
  location    = {Oakland, United States},
  publisher   = {Association for Computational Lingustics},
  url         = {https://aclanthology.org/2024.nlp4musa-1.12/},
  pages       = {69--74},
  abstract    = {Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language Processing to build a vocabulary of subwords, which has been recently applied to symbolic music. Given that symbolic music can differ significantly from text, particularly with polyphony, we investigate how BPE behaves with different types of musical content. This study provides a qualitative analysis of BPE`s behavior across various instrumentations and evaluates its impact on a musical phrase segmentation task for both monophonic and polyphonic music. Our findings show that the BPE training process is highly dependent on the instrumentation and that BPE {\textquotedblleft}supertokens{\textquotedblright} succeed in capturing abstract musical content. In a musical phrase segmentation task, BPE notably improves performance in a polyphonic setting, but enhances performance in monophonic tunes only within a specific range of BPE merges.},
  abbr        = {Symbolic<br>Music Analysis},
  pdf         = {https://aclanthology.org/2024.nlp4musa-1.12.pdf},
  code        = {https://github.com/dinhviettoanle/musicbpe-mono-poly},
  conf_web    = {https://sites.google.com/view/nlp4musa-2024},
  preview     = {bpe.png}
}


@inproceedings{le2025evaluating,
  bibtex_show = {true},
  title       = {Evaluating Interval-based Tokenization for Pitch Representation in Symbolic Music Analysis},
  author      = {Le, Dinh-Viet-Toan and Bigo, Louis and Keller, Mikaela},
  year        = 2025,
  month       = Mar,
  booktitle   = {Workshop Artificial Intelligence for Music at AAAI},
  location    = {Philadelphia, United States},
  abstract    = {Symbolic music analysis tasks are often performed by models originally developed for Natural Language Processing, such as Transformers. Such models require the input data to be represented as sequences, which is achieved through a process of tokenization. Tokenization strategies for symbolic music often rely on absolute MIDI values to represent pitch information. However, music research largely promotes the benefit of higher-level representations such as melodic contour and harmonic relations for which pitch intervals turn out to be more expressive than absolute pitches. In this work, we introduce a general framework for building interval-based tokenizations. By evaluating these tokenizations on three music analysis tasks, we show that such interval-based tokenizations improve model performances and facilitate their explainability},
  abbr        = {Symbolic<br>Music Analysis},
  preview     = {intervalization.png},
  pdf         = {https://hal.science/hal-04877659v1/document},
  conf_web    = {https://ai4musicians.org/2025aaai.html}
}

misc{le2024meteor,
  bibtex_show   = {true},
  title         = {METEOR: Melody-aware Texture-controllable Symbolic Orchestral Music Generation},
  author        = {Dinh-Viet-Toan Le and Yi-Hsuan Yang},
  year          = {2024},
  month         = Sep,
  eprint        = {2409.11753},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD},
  abstract      = {Western music is often characterized by a homophonic texture, in which the musical content can be organized into a melody and an accompaniment. In orchestral music, in particular, the composer can select specific characteristics for each instrument's part within the accompaniment, while also needing to adapt the melody to suit the capabilities of the instruments performing it. In this work, we propose METEOR, a model for Melody-aware Texture-controllable Orchestral music generation. This model performs symbolic multi-track music style transfer with a focus on melodic fidelity. We allow bar- and track-level controllability of the accompaniment with various textural attributes while keeping a homophonic texture. We show that the model can achieve controllability performances similar to strong baselines while greatly improve melodic fidelity.},
  abbr          = {Symbolic<br>Music Generation},
  pdf           = {https://arxiv.org/pdf/2409.11753},
  code          = {https://github.com/dinhviettoanle/meteor},
  demo          = {https://dinhviettoanle.github.io/meteor/},
  preview       = {meteor.png}
}

inproceedings{le2025meteor,
  bibtex_show = {true},
  title       = {{METEOR}: Melody-aware Texture-controllable Symbolic Music Re-Orchestration via Transformer VAE},
  author      = {Dinh-Viet-Toan Le and Yi-Hsuan Yang},
  booktitle   = {Proceedings of the 34th International Joint Conference on Artificial Intelligence (IJCAI), Special Track on AI, Arts and Creativity},
  publisher   = {International Joint Conferences on Artificial Intelligence Organization},
  pages       = {},
  year        = 2025,
  month       = aug,
  location    = {Montreal, Canada},
  url         = {https://arxiv.org/abs/2409.11753},
  abstract    = {Re-orchestration is the process of adapting a music piece for a different set of instruments. By altering the original instrumentation, the orchestrator often modifies the musical texture while preserving a recognizable melodic line and ensures that each part is playable within the technical and expressive capabilities of the chosen instruments. In this work, we propose METEOR, a model for generating Melody-aware Texture-controllable re-Orchestration with a Transformer-based variational auto-encoder (VAE). This model performs symbolic instrumental and textural music style transfers with a focus on melodic fidelity and controllability. We allow bar- and track-level controllability of the accompaniment with various textural attributes while keeping a homophonic texture. With both subjective and objective evaluations, we show that our model outperforms style transfer models on a re-orchestration task in terms of generation quality and controllability. Moreover, it can be adapted for a lead sheet orchestration task as a zero-shot learning model, achieving performance comparable to a model specifically trained for this task.},
  abbr        = {Symbolic<br>Music Generation},
  pdf         = {https://arxiv.org/pdf/2409.11753},
  code        = {https://github.com/dinhviettoanle/meteor},
  demo        = {https://dinhviettoanle.github.io/meteor/},
  preview     = {meteor.png},
  conf_web    = {https://2025.ijcai.org/},
}

@inproceedings{le2025meteor,
  title     = {METEOR: Melody-aware Texture-controllable Symbolic Music Re-Orchestration via Transformer VAE},
  author    = {Le, Dinh-Viet-Toan and Yang, Yi-Hsuan},
  booktitle = {Proceedings of the Thirty-Fourth International Joint Conference on
               Artificial Intelligence, {IJCAI-25}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {James Kwok},
  pages     = {10126--10134},
  year      = 2025,
  month     = aug,
  note      = {AI, Arts & Creativity},
  doi       = {10.24963/ijcai.2025/1125},
  address    = {Montreal, Canada},
  url       = {https://doi.org/10.24963/ijcai.2025/1125},
  abstract    = {Re-orchestration is the process of adapting a music piece for a different set of instruments. By altering the original instrumentation, the orchestrator often modifies the musical texture while preserving a recognizable melodic line and ensures that each part is playable within the technical and expressive capabilities of the chosen instruments. In this work, we propose METEOR, a model for generating Melody-aware Texture-controllable re-Orchestration with a Transformer-based variational auto-encoder (VAE). This model performs symbolic instrumental and textural music style transfers with a focus on melodic fidelity and controllability. We allow bar- and track-level controllability of the accompaniment with various textural attributes while keeping a homophonic texture. With both subjective and objective evaluations, we show that our model outperforms style transfer models on a re-orchestration task in terms of generation quality and controllability. Moreover, it can be adapted for a lead sheet orchestration task as a zero-shot learning model, achieving performance comparable to a model specifically trained for this task.},
  abbr        = {Symbolic<br>Music Generation},
  pdf         = {https://arxiv.org/pdf/2409.11753},
  code        = {https://github.com/dinhviettoanle/meteor},
  demo        = {https://dinhviettoanle.github.io/meteor/},
  preview     = {meteor.png},
  conf_web    = {https://2025.ijcai.org/},
}


misc{le2024natural,
  bibtex_show   = {true},
  title         = {Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A Survey},
  author        = {Dinh-Viet-Toan Le and Louis Bigo and Mikaela Keller and Dorien Herremans},
  year          = 2024,
  month         = Feb,
  eprint        = {2402.17467},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  abbr          = {Survey},
  pdf           = {https://arxiv.org/abs/2402.17467},
  code          = {https://github.com/dinhviettoanle/survey-music-nlp},
  preview       = {survey.png}
}

article{le2025natural,
  bibtex_show = {true},
  author    = {Le, Dinh-Viet-Toan and Bigo, Louis and Herremans, Dorien and Keller, Mikaela},
  title     = {Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A Survey},
  year      = {2025},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  issn      = {0360-0300},
  url       = {https://doi.org/10.1145/3714457},
  doi       = {10.1145/3714457},
  abstract  = {Music is frequently associated with the notion of language as both domains share several similarities, including the ability for their content to be represented as sequences of symbols. In computer science, the fields of Natural Language Processing (NLP) and Music Information Retrieval (MIR) reflect this analogy through a variety of similar tasks, such as author detection or content generation. This similarity has long encouraged the adaptation of NLP methods to process musical data, in particular symbolic music data, and the rise of Transformer neural networks has considerably strengthened this practice. This survey reviews NLP methods applied to symbolic music generation and information retrieval following two axes. We first propose an overview of representations of symbolic music inspired by text sequential representations. We then review a large set of computational models, in particular deep learning models, that have been adapted from NLP to process these musical representations for various MIR tasks. These models are described and categorized through different prisms with a highlight on their music-specialized mechanisms. We finally present a discussion surrounding the adequate use of NLP tools to process symbolic music data. This includes technical issues regarding NLP methods which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.},
  note      = {Just Accepted},
  journal   = {ACM Computing Surveys},
  month     = jan,
  keywords  = {Music Information Retrieval, Natural Language Processing, Symbolic music, Music generation, Music analysis, Deep learning.},
  abbr        = {Survey},
  pdf         = {https://arxiv.org/pdf/2402.17467},
  code        = {https://github.com/dinhviettoanle/survey-music-nlp},
  preview     = {survey.png},
}

@article{le2025natural,
  bibtex_show = {true},
  author      = {Le, Dinh-Viet-Toan and Bigo, Louis and Herremans, Dorien and Keller, Mikaela},
  title       = {Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A Survey},
  year        = 2025,
  issue_date  = {July 2025},
  publisher   = {Association for Computing Machinery},
  address     = {New York, NY, USA},
  volume      = {57},
  number      = {7},
  issn        = {0360-0300},
  url         = {https://doi.org/10.1145/3714457},
  doi         = {10.1145/3714457},
  abstract    = {Music is frequently associated with the notion of language, as both domains share several similarities, including the ability for their content to be represented as sequences of symbols. In computer science, the fields of Natural Language Processing (NLP) and Music Information Retrieval (MIR) reflect this analogy through a variety of similar tasks, such as author detection or content generation. This similarity has long encouraged the adaptation of NLP methods to process musical data, particularly symbolic music data, and the rise of Transformer neural networks has considerably strengthened this practice. This survey reviews NLP methods applied to symbolic music generation and information retrieval following two axes. We first propose an overview of representations of symbolic music inspired by text sequential representations. We then review a large set of computational models, particularly deep learning models, which have been adapted from NLP to process these musical representations for various MIR tasks. These models are described and categorized through different prisms with a highlight on their music-specialized mechanisms. We finally present a discussion surrounding the adequate use of NLP tools to process symbolic music data. This includes technical issues regarding NLP methods which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.},
  journal     = {ACM Computing Surveys},
  month       = feb,
  articleno   = {175},
  numpages    = {40},
  keywords    = {Music information retrieval, natural language processing, symbolic music, music generation, music analysis, deep learning},
  abbr        = {Survey},
  pdf         = {https://dl.acm.org/doi/pdf/10.1145/3714457},
  code        = {https://github.com/dinhviettoanle/survey-music-nlp},
  preview     = {survey.png}
}


@inproceedings{le2022orchestral,
  bibtex_show = {true},
  title       = {A Corpus Describing Orchestral Texture in First Movements of Classical and Early-Romantic Symphonies},
  author      = {Le, Dinh-Viet-Toan and Giraud, Mathieu and Lev\'{e}, Florence and Maccarini, Francesco},
  year        = 2022,
  month       = jul,
  booktitle   = {Proceedings of the 9th International Conference on Digital Libraries for Musicology},
  pages       = {27-35},
  location    = {Prague, Czech Republic},
  publisher   = {Association for Computing Machinery},
  address     = {New York, NY, USA},
  isbn        = 9781450396684,
  url         = {https://doi.org/10.1145/3543882.3543884},
  doi         = {10.1145/3543882.3543884},
  abstract    = {Orchestration is the art of writing music for a possibly large ensemble of instruments, by blending or opposing their sounds and grouping them into an orchestral texture. We aim here at providing a deeper understanding of orchestration in classical and early-romantic symphonies by analyzing, at the bar level, how the instruments of the orchestra organize into melodic, rhythmic, harmonic, and mixed layers. We formalize the description of such layers and release an open corpus with more than 7900&nbsp;annotations in 24&nbsp;first movements of Haydn, Mozart, and Beethoven symphonies. Initial analyses of this corpus confirm specific roles of the instruments and their families (woodwinds, brass, and strings), some evolution between composers, as well as the contribution of orchestral texture to form. The model and the corpus offer perspectives for empirical and computational studies on orchestral music.},
  numpages    = 9,
  keywords    = {corpus, layers, music texture, orchestration, symbolic data},
  series      = {DLfM '22},
  abbr        = {Musicology},
  pdf         = {https://hal.science/hal-03663112/document},
  code        = {https://gitlab.com/algomus.fr/orchestration},
  conf_web    = {https://dlfm.web.ox.ac.uk/9th-international-conference-on-digital-libraries-for-musicology},
  preview     = {orchestral_texture.png}
}

@article{bigo2024langage,
  bibtex_show = {true},
  title       = {Le langage des partitions musicales face {\`a} l'intelligence artificielle},
  author      = {Bigo, Louis and Keller, Mikaela and Le, Dinh-Viet-Toan},
  year        = 2024,
  month       = nov,
  journal     = {Culture et recherche},
  pages       = {34--35},
  publisher   = {Paris : Minist{\`e}re de la Culture et de la Communication},
  url         = {https://hal.science/hal-04909478},
  abstract    = {Dans le domaine de l'informatique musicale, l'analyse et la génération automatique de partitions font l'objet de multiples recherches, en grande partie inspirées par des algorithmes d'Intelligence artificielle (IA) conçus à l'origine pour le traitement de contenus textuels. Le projet Music NLP, collaboration entre le Studio de création et de recherche en informatique et musiques expérimentales (SCRIME) à l'Université de Bordeaux, l'Institut national de recherche en sciences et technologies du numérique (INRIA) et le Centre de recherche en informatique signal et automatique de Lille (CRIStAL) à l'Université de Lille, mène une réflexion sur ce détournement d'outils, ses limites techniques et la manière dont il nous interroge à un plus haut niveau sur le parallèle entre musique et langage naturel. },
  keywords    = {Intelligence artificielle ; partitions musicales ; analyse musicale ; composition musicale ; langage naturel},
  hal_id      = {hal-04909478},
  hal_version = {v1},
  abbr        = {General public},
  pdf         = {https://hal.science/hal-04909478/document},
  preview     = {culture_recherche.png}
}
